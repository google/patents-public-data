{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add header/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "GPU_MEM_CONFIG = tf.ConfigProto(gpu_options={'allow_growth': True})\n",
    "seed_file = 'seeds/3d_gesture.seed.csv'\n",
    "#seed_file = 'seeds/dnn.seed.csv'\n",
    "# BigQuery must be enabled for this project\n",
    "bq_project = 'patent-landscape-165715'\n",
    "patent_dataset = 'patents-public-data:patents.publications_latest'\n",
    "num_anti_seed_patents = 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use models/5.9m directory to load/persist model information.\n",
      "INFO:tensorflow:Restoring parameters from models/5.9m/checkpoints/5.9m_abstracts.ckpt-1325000\n"
     ]
    }
   ],
   "source": [
    "from expansion import PatentLandscapeExpander\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "expander = PatentLandscapeExpander(\n",
    "    seed_file,\n",
    "    bq_project=bq_project,\n",
    "    patent_dataset=patent_dataset,\n",
    "    num_antiseed=num_anti_seed_patents)\n",
    "\n",
    "word2vec5_9m = Word2Vec('5.9m')\n",
    "w2v_runtime = word2vec5_9m.restore_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance': 0.18873202302828984, 'index': 6283, 'word': 'cars'},\n",
       " {'distance': 0.27777841887585764, 'index': 3112, 'word': 'elevator'},\n",
       " {'distance': 0.28295611080718119, 'index': 6000, 'word': 'railway'},\n",
       " {'distance': 0.30767008708779853, 'index': 6287, 'word': 'railroad'},\n",
       " {'distance': 0.36829579541764967, 'index': 9308, 'word': 'freight'},\n",
       " {'distance': 0.40134100959237196, 'index': 8236, 'word': 'floors'},\n",
       " {'distance': 0.4137411535080775, 'index': 8113, 'word': 'passengers'},\n",
       " {'distance': 0.43070808547133022, 'index': 2273, 'word': 'truck'},\n",
       " {'distance': 0.43651028262115854, 'index': 7628, 'word': 'trucks'},\n",
       " {'distance': 0.43760085223752299, 'index': 15259, 'word': 'hoistway'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_embedding = w2v_runtime.load_embedding('test')\n",
    "w2v_runtime.find_similar('car', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for all US CPC Counts\n",
      "Querying for Seed Set CPC Counts\n",
      "Querying to find total number of US patents\n",
      "Got 26282 relevant seed refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (26282, 1), to patents._l1_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L1 expansion: (133137, 3)\n",
      "Got 522720 relevant L1->L2 refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (522720, 1), to patents._l2_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L2 expansion: (495565, 3)\n",
      "Size of union of [Seed, L1, and L2]: 547057\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (547057, 1), to patents.antiseed_tmp\n",
      "Completed loading temp table.\n",
      "Loading training data text from (16167, 2) publication numbers\n",
      "Loading dataframe with cols Index(['publication_number'], dtype='object'), shape (16167, 1), to patents._tmp_training\n",
      "Completed loading temp table.\n",
      "Loading patent texts from provided publication numbers.\n",
      "Merging labels into training data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_num</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>family_id</th>\n",
       "      <th>priority_date</th>\n",
       "      <th>title_text</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>description_text</th>\n",
       "      <th>refs</th>\n",
       "      <th>cpcs</th>\n",
       "      <th>ExpansionLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005037026</td>\n",
       "      <td>US-2005037026-A1</td>\n",
       "      <td>26748211</td>\n",
       "      <td>19971202</td>\n",
       "      <td>Prevention and treatment of amyloidogenic disease</td>\n",
       "      <td>The invention provides compositions and method...</td>\n",
       "      <td>1 . A composition comprising a fragment of Aβ ...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATIONS  \\n   ...</td>\n",
       "      <td>US-4666829-A,US-4666829-A,US-4666829-A,US-4666...</td>\n",
       "      <td>A61K2039/505,A61K9/0019,A61K47/4833,A61K9/2054...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8045948</td>\n",
       "      <td>US-8045948-B2</td>\n",
       "      <td>40088873</td>\n",
       "      <td>20070531</td>\n",
       "      <td>Receiving apparatus, program and receiving method</td>\n",
       "      <td>A receiving apparatus includes a receiving por...</td>\n",
       "      <td>1. A receiving apparatus, comprising:\\n a rece...</td>\n",
       "      <td>CROSS REFERENCES TO RELATED APPLICATIONS \\n   ...</td>\n",
       "      <td>US-6477196-B1,US-2008298517-A1,US-7729679-B1</td>\n",
       "      <td>H04B1/001,H04B1/001,H04B1/001</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005116101</td>\n",
       "      <td>US-2005116101-A1</td>\n",
       "      <td>34590269</td>\n",
       "      <td>20031107</td>\n",
       "      <td>Emergency oxygen or other gas supply system</td>\n",
       "      <td>Systems for supplying emergency oxygen or othe...</td>\n",
       "      <td>1 . A vehicle seat assembly comprising: \\n a. ...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...</td>\n",
       "      <td>US-2931355-A,US-2931355-A,US-2931355-A,US-2931...</td>\n",
       "      <td>B64D11/00,B64D11/0629,B64D11/064,B64D2231/025,...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3817657</td>\n",
       "      <td>US-3817657-A</td>\n",
       "      <td>5824478</td>\n",
       "      <td>19711108</td>\n",
       "      <td>Integral turbine wheel with axial through-open...</td>\n",
       "      <td>A turbine wheel in which the rim and the blade...</td>\n",
       "      <td>1. A turbine wheel having rim means and blade ...</td>\n",
       "      <td>United States Patent [191 Hueber [451 June 18,...</td>\n",
       "      <td>US-2965355-A,US-2965355-A,US-3255994-A,US-3255...</td>\n",
       "      <td>F01D5/021,F01D5/081,F01D5/021,F01D5/081,F01D5/...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3853646</td>\n",
       "      <td>US-3853646-A</td>\n",
       "      <td>24524820</td>\n",
       "      <td>19670405</td>\n",
       "      <td>Smokeless composite propellants containing car...</td>\n",
       "      <td>Smokeless rocket propellant compositions compr...</td>\n",
       "      <td>1. SMOKELESS ROCKET PROPELLANT COMPOSITIONS CO...</td>\n",
       "      <td>fitted Sites Patent 1191 Frankel et a1.  \\n [ ...</td>\n",
       "      <td>US-3097239-A,US-3097239-A,US-3087844-A,US-3087...</td>\n",
       "      <td>C06B45/10,C06B43/00,C06B45/10,C06B43/00,C06B45...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_num publication_number family_id  priority_date  \\\n",
       "0  2005037026   US-2005037026-A1  26748211       19971202   \n",
       "1     8045948      US-8045948-B2  40088873       20070531   \n",
       "2  2005116101   US-2005116101-A1  34590269       20031107   \n",
       "3     3817657       US-3817657-A   5824478       19711108   \n",
       "4     3853646       US-3853646-A  24524820       19670405   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  Prevention and treatment of amyloidogenic disease   \n",
       "1  Receiving apparatus, program and receiving method   \n",
       "2        Emergency oxygen or other gas supply system   \n",
       "3  Integral turbine wheel with axial through-open...   \n",
       "4  Smokeless composite propellants containing car...   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  The invention provides compositions and method...   \n",
       "1  A receiving apparatus includes a receiving por...   \n",
       "2  Systems for supplying emergency oxygen or othe...   \n",
       "3  A turbine wheel in which the rim and the blade...   \n",
       "4  Smokeless rocket propellant compositions compr...   \n",
       "\n",
       "                                         claims_text  \\\n",
       "0  1 . A composition comprising a fragment of Aβ ...   \n",
       "1  1. A receiving apparatus, comprising:\\n a rece...   \n",
       "2  1 . A vehicle seat assembly comprising: \\n a. ...   \n",
       "3  1. A turbine wheel having rim means and blade ...   \n",
       "4  1. SMOKELESS ROCKET PROPELLANT COMPOSITIONS CO...   \n",
       "\n",
       "                                    description_text  \\\n",
       "0  CROSS-REFERENCE TO RELATED APPLICATIONS  \\n   ...   \n",
       "1  CROSS REFERENCES TO RELATED APPLICATIONS \\n   ...   \n",
       "2  CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...   \n",
       "3  United States Patent [191 Hueber [451 June 18,...   \n",
       "4  fitted Sites Patent 1191 Frankel et a1.  \\n [ ...   \n",
       "\n",
       "                                                refs  \\\n",
       "0  US-4666829-A,US-4666829-A,US-4666829-A,US-4666...   \n",
       "1       US-6477196-B1,US-2008298517-A1,US-7729679-B1   \n",
       "2  US-2931355-A,US-2931355-A,US-2931355-A,US-2931...   \n",
       "3  US-2965355-A,US-2965355-A,US-3255994-A,US-3255...   \n",
       "4  US-3097239-A,US-3097239-A,US-3087844-A,US-3087...   \n",
       "\n",
       "                                                cpcs ExpansionLevel  \n",
       "0  A61K2039/505,A61K9/0019,A61K47/4833,A61K9/2054...       AntiSeed  \n",
       "1                      H04B1/001,H04B1/001,H04B1/001       AntiSeed  \n",
       "2  B64D11/00,B64D11/0629,B64D11/064,B64D2231/025,...       AntiSeed  \n",
       "3  F01D5/021,F01D5/081,F01D5/021,F01D5/081,F01D5/...       AntiSeed  \n",
       "4  C06B45/10,C06B43/00,C06B45/10,C06B43/00,C06B45...       AntiSeed  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_full_df, seed_patents_df, l1_patents_df, l2_patents_df, anti_seed_patents = \\\n",
    "    expander.derive_training_data_from_seeds(seed_file)\n",
    "training_data_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = training_data_full_df[\n",
    "    ['publication_number', 'title_text', 'abstract_text', 'claims_text', 'description_text', 'ExpansionLevel', 'refs', 'cpcs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed/Positive examples:\n",
      "publication_number    1092\n",
      "title_text            1092\n",
      "abstract_text         1092\n",
      "claims_text           1092\n",
      "description_text      1092\n",
      "ExpansionLevel        1092\n",
      "refs                  1092\n",
      "cpcs                  1092\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Anti-Seed/Negative examples:\n",
      "publication_number    9582\n",
      "title_text            9582\n",
      "abstract_text         9582\n",
      "claims_text           9582\n",
      "description_text      9582\n",
      "ExpansionLevel        9582\n",
      "refs                  9582\n",
      "cpcs                  9582\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Seed/Positive examples:')\n",
    "print(training_df[training_df.ExpansionLevel == 'Seed'].count())\n",
    "\n",
    "print('\\n\\nAnti-Seed/Negative examples:')\n",
    "print(training_df[training_df.ExpansionLevel == 'AntiSeed'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomizing training data\n",
      "Creating NumPy arrays for train/test set out of randomized training data.\n",
      "Train (embed) data shapes: train: (8539,), train labels shape: (8539,)\n",
      "Test (embed) data shape: (2135,), test labels shape: (2135,)\n",
      "doc lengths for embedding layer: median: 103, mean: 105.21641878440099, max: 3546\n",
      "Using sequence length of 3546 to pad LSTM sequences.\n",
      "Training data ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<train_data.LandscapeTrainingData at 0x7ff08371d710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import train_data\n",
    "\n",
    "#importlib.reload(tokenizer)\n",
    "\n",
    "td = train_data.LandscapeTrainingData(training_df, w2v_runtime)\n",
    "td.prepare_training_data(training_df.abstract_text, 0.8, 50000, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: The invention provides compositions and methods for treatment of amyloidogenic diseases. Such methods entail administering an agent that induces a beneficial immune response against an amyloid deposit in the patient. The methods are particularly useful for prophylactic and therapeutic treatment of Alzheimer&#39;s disease. In such methods, a suitable agent is Aβ peptide or an antibody thereto.\n",
      "Tokenized: invention provides compositions and methods for treatment of amyloidogenic diseases such methods entail administering an agent that induces a beneficial immune response against an amyloid deposit in patient methods are particularly useful for prophylactic and therapeutic treatment of alzheimer _NUMBER_ s disease in such methods a suitable agent is UNK peptide or an antibody thereto\n",
      "Integerized: [45, 135, 400, 3, 155, 9, 401, 2, 36877, 1681, 42, 155, 20187, 2594, 8, 374, 19, 6564, 1, 6146, 3926, 230, 317, 8, 9648, 3212, 6, 606, 155, 13, 507, 442, 9, 8714, 3, 1882, 401, 2, 6804, 7, 140, 1870, 6, 42, 155, 1, 552, 374, 5, 110239, 2645, 11, 8, 2691, 782]\n",
      "LabelIntegerized: 1\n"
     ]
    }
   ],
   "source": [
    "def show_instance_details(training_data, training_data_series, idx):\n",
    "    print('\\nOriginal: {}\\nTokenized: {}\\nIntegerized: {}\\nLabelIntegerized: {}'.format(\n",
    "        training_data_series[idx],\n",
    "        training_data.to_text(training_data.prepped_embedding_train[idx]),\n",
    "        training_data.prepped_embedding_train[idx],\n",
    "        training_data.prepped_labels[idx]))\n",
    "\n",
    "show_instance_details(td, td.series_text_to_embed, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index 29.\n",
      "[1, 375, 378, 9, 4908, 10267, 69, 1, 375, 778, 9, 2595, 8804, 2, 1, 124, 75, 778, 7, 140, 269, 2, 1316, 6, 1295, 83, 8, 59, 378, 166, 10, 375, 778, 9, 365, 1, 47, 2, 777, 10267, 15, 8804, 2178, 12, 375, 778, 3, 1, 276, 166, 10, 59, 378, 79, 276, 38, 20, 6, 1, 14195, 266, 11, 8, 327, 266, 3, 79, 276, 4097, 15, 14195, 266, 4, 327, 266, 52, 59, 378, 1228, 1, 7349, 2, 56, 1858, 75, 2178, 8804, 56, 1858, 65, 262, 1, 405, 17505, 3, 1087, 8, 303, 777, 3, 79, 276, 1481, 1, 2252, 849, 875, 4, 8, 286, 25, 91, 17, 777, 10267, 493, 12, 59, 378, 52, 276, 5, 6, 327, 266, 1, 30, 3, 1, 225, 2359, 165, 272, 13, 61, 309, 3, 4229]\n",
      "a video processor for recognizing gestures including a video camera for capturing photographs of a region within camera _NUMBER_ s field of view in real time an image processor coupled with video camera for detecting a plurality of hand gestures from photographs captured by video camera and a controller coupled with image processor wherein controller can be in a dormant mode or an active mode and wherein controller transitions from dormant mode to active mode when image processor detects a progression of two states within captured photographs two states being i a closed fist and ii an open hand and wherein controller performs a programmed responsive action to an electronic device based on hand gestures detected by image processor when controller is in active mode a method and a computer readable storage medium are also described and claimed\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# finding a positive example and showing some details\n",
    "print_idx = 0\n",
    "for row in td.trainY:\n",
    "    if td.trainY[print_idx] == 0:\n",
    "        break\n",
    "    else:\n",
    "        print_idx += 1\n",
    "\n",
    "print('Using index {}.'.format(print_idx))\n",
    "print(td.trainEmbedX[print_idx])\n",
    "print(td.to_text(td.trainEmbedX[print_idx]))\n",
    "print(td.trainY[print_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dropout_pct = 0.2\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/ipykernel/__main__.py:65: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 8576 samples, validate on 2145 samples\n",
      "Epoch 1/5\n",
      "8576/8576 [==============================] - 142s - loss: 0.1001 - acc: 0.9665 - precision: 0.9848 - recall: 0.9774 - fmeasure: 0.9794 - val_loss: 0.0569 - val_acc: 0.9874 - val_precision: 0.9865 - val_recall: 1.0000 - val_fmeasure: 0.9931\n",
      "Epoch 2/5\n",
      "8576/8576 [==============================] - 144s - loss: 0.0397 - acc: 0.9868 - precision: 0.9913 - recall: 0.9941 - fmeasure: 0.9926 - val_loss: 0.0308 - val_acc: 0.9893 - val_precision: 0.9924 - val_recall: 0.9959 - val_fmeasure: 0.9940\n",
      "Epoch 3/5\n",
      "8576/8576 [==============================] - 143s - loss: 0.0184 - acc: 0.9935 - precision: 0.9952 - recall: 0.9976 - fmeasure: 0.9963 - val_loss: 0.0329 - val_acc: 0.9888 - val_precision: 0.9889 - val_recall: 0.9989 - val_fmeasure: 0.9938\n",
      "Epoch 4/5\n",
      "8576/8576 [==============================] - 144s - loss: 0.0130 - acc: 0.9960 - precision: 0.9970 - recall: 0.9985 - fmeasure: 0.9977 - val_loss: 0.0466 - val_acc: 0.9855 - val_precision: 0.9864 - val_recall: 0.9979 - val_fmeasure: 0.9920\n",
      "Epoch 5/5\n",
      "8576/8576 [==============================] - 143s - loss: 0.0092 - acc: 0.9976 - precision: 0.9978 - recall: 0.9994 - fmeasure: 0.9985 - val_loss: 0.0309 - val_acc: 0.9902 - val_precision: 0.9915 - val_recall: 0.9979 - val_fmeasure: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea4072beb8>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "refs = Sequential()\n",
    "refs.add(\n",
    "    Dense(\n",
    "        256,\n",
    "        input_dim=td.trainRefsOneHotX.shape[1],\n",
    "        name='refs',\n",
    "        activation=None))\n",
    "refs.add(Dropout(dropout_pct))\n",
    "refs.add(BatchNormalization())\n",
    "refs.add(ELU())\n",
    "refs.add(Dense(64, activation=None))\n",
    "refs.add(Dropout(dropout_pct))\n",
    "refs.add(BatchNormalization())\n",
    "refs.add(ELU())\n",
    "\n",
    "cpcs = Sequential()\n",
    "cpcs.add(\n",
    "    Dense(\n",
    "        32,\n",
    "        input_dim=td.trainCpcOneHotX.shape[1],\n",
    "        name='cpcs',\n",
    "        activation=None))\n",
    "cpcs.add(Dropout(dropout_pct))\n",
    "cpcs.add(BatchNormalization())\n",
    "cpcs.add(ELU())\n",
    "\n",
    "deep = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(td.w2v_runtime.embedding_weights.shape[0],\n",
    "                            td.w2v_runtime.embedding_weights.shape[1],\n",
    "                            weights=[td.w2v_runtime.embedding_weights],\n",
    "                            #input_length=sequence_len,\n",
    "                            trainable=False,\n",
    "                            name='embed')\n",
    "deep.add(embedding_layer)\n",
    "'''\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "'''\n",
    "deep.add(LSTM(\n",
    "    lstm_size,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=False,\n",
    "    name='LSTM_1'))\n",
    "#model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, name='LSTM_2'))\n",
    "deep.add(Dense(300, activation=None))\n",
    "deep.add(Dropout(dropout_pct))\n",
    "deep.add(BatchNormalization())\n",
    "deep.add(ELU())\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(concatenate([wide, deep], axis=1))\n",
    "model.add(Merge([refs, cpcs, deep], mode='concat', concat_axis=1))\n",
    "cpcs.add(Dense(64, activation=None))\n",
    "cpcs.add(Dropout(dropout_pct))\n",
    "cpcs.add(BatchNormalization())\n",
    "cpcs.add(ELU())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "#for seq, label in zip(trainX, trainY):\n",
    "#    print('fitting\\n seq: {} \\n label: {}\\n'.format(seq, label))\n",
    "#    model.train_on_batch(np.array([seq]), [label])\n",
    "\n",
    "model.fit(x={\n",
    "            'refs_input': td.trainRefsOneHotX,\n",
    "            'embed_input': td.padded_train_embed_x,\n",
    "            'cpcs_input': td.trainCpcOneHotX},\n",
    "          y=td.trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(\n",
    "              {\n",
    "                  'refs_input': td.testRefsOneHotX,\n",
    "                  'cpcs_input': td.testCpcOneHotX,\n",
    "                  'embed_input': td.padded_test_embed_x},\n",
    "              td.testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2145/2145 [==============================] - 7s     \n",
      "\n",
      "Test score: 0.0665\n",
      "Test accuracy: 0.9776\n",
      "Test p/r (f1): 0.98/1.00 (0.99)\n"
     ]
    }
   ],
   "source": [
    "score, acc, p, r, f1 = model.evaluate(\n",
    "    x={'refs_input': td.testRefsOneHotX, 'embed_input': td.padded_test_embed_x},\n",
    "    y=test_y,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "print('')\n",
    "print('Test score: {:.4f}'.format(score))\n",
    "print('Test accuracy: {:.4f}'.format(acc))\n",
    "print('Test p/r (f1): {:.2f}/{:.2f} ({:.2f})'.format(p, r, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow LSTM Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "vocab_size = w2v_runtime.embedding_weights.shape[0]\n",
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = w2v_runtime.embedding_weights.shape[1]\n",
    "\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    embedding = tf.get_variable(\n",
    "        name=\"embedding\",\n",
    "        shape=w2v_runtime.embedding_weights.shape,\n",
    "        initializer=tf.constant_initializer(w2v_runtime.embedding_weights),\n",
    "        trainable=False)\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.134\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.102\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.131\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.086\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.121\n",
      "Val acc: 0.884\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.083\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.064\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.211\n",
      "Epoch: 0/10 Iteration: 45 Train loss: 0.101\n",
      "Epoch: 0/10 Iteration: 50 Train loss: 0.065\n",
      "Val acc: 0.910\n",
      "Epoch: 0/10 Iteration: 55 Train loss: 0.092\n",
      "Epoch: 0/10 Iteration: 60 Train loss: 0.051\n",
      "Epoch: 0/10 Iteration: 65 Train loss: 0.053\n",
      "Epoch: 0/10 Iteration: 70 Train loss: 0.037\n",
      "Epoch: 0/10 Iteration: 75 Train loss: 0.038\n",
      "Val acc: 0.933\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.039\n",
      "Epoch: 1/10 Iteration: 85 Train loss: 0.079\n",
      "Epoch: 1/10 Iteration: 90 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 95 Train loss: 0.033\n",
      "Epoch: 1/10 Iteration: 100 Train loss: 0.042\n",
      "Val acc: 0.949\n",
      "Epoch: 1/10 Iteration: 105 Train loss: 0.057\n",
      "Epoch: 1/10 Iteration: 110 Train loss: 0.068\n",
      "Epoch: 1/10 Iteration: 115 Train loss: 0.067\n",
      "Epoch: 1/10 Iteration: 120 Train loss: 0.079\n",
      "Epoch: 1/10 Iteration: 125 Train loss: 0.049\n",
      "Val acc: 0.940\n",
      "Epoch: 1/10 Iteration: 130 Train loss: 0.049\n",
      "Epoch: 1/10 Iteration: 135 Train loss: 0.041\n",
      "Epoch: 1/10 Iteration: 140 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 145 Train loss: 0.148\n",
      "Epoch: 1/10 Iteration: 150 Train loss: 0.072\n",
      "Val acc: 0.940\n",
      "Epoch: 2/10 Iteration: 155 Train loss: 0.066\n",
      "Epoch: 2/10 Iteration: 160 Train loss: 0.056\n",
      "Epoch: 2/10 Iteration: 165 Train loss: 0.049\n",
      "Epoch: 2/10 Iteration: 170 Train loss: 0.036\n",
      "Epoch: 2/10 Iteration: 175 Train loss: 0.030\n",
      "Val acc: 0.942\n",
      "Epoch: 2/10 Iteration: 180 Train loss: 0.053\n",
      "Epoch: 2/10 Iteration: 185 Train loss: 0.038\n",
      "Epoch: 2/10 Iteration: 190 Train loss: 0.029\n",
      "Epoch: 2/10 Iteration: 195 Train loss: 0.066\n",
      "Epoch: 2/10 Iteration: 200 Train loss: 0.039\n",
      "Val acc: 0.936\n",
      "Epoch: 2/10 Iteration: 205 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 210 Train loss: 0.057\n",
      "Epoch: 2/10 Iteration: 215 Train loss: 0.103\n",
      "Epoch: 2/10 Iteration: 220 Train loss: 0.064\n",
      "Epoch: 2/10 Iteration: 225 Train loss: 0.062\n",
      "Val acc: 0.931\n",
      "Epoch: 3/10 Iteration: 230 Train loss: 0.053\n",
      "Epoch: 3/10 Iteration: 235 Train loss: 0.050\n",
      "Epoch: 3/10 Iteration: 240 Train loss: 0.057\n",
      "Epoch: 3/10 Iteration: 245 Train loss: 0.037\n",
      "Epoch: 3/10 Iteration: 250 Train loss: 0.112\n",
      "Val acc: 0.560\n",
      "Epoch: 3/10 Iteration: 255 Train loss: 0.072\n",
      "Epoch: 3/10 Iteration: 260 Train loss: 0.046\n",
      "Epoch: 3/10 Iteration: 265 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 270 Train loss: 0.053\n",
      "Epoch: 3/10 Iteration: 275 Train loss: 0.030\n",
      "Val acc: 0.947\n",
      "Epoch: 3/10 Iteration: 280 Train loss: 0.035\n",
      "Epoch: 3/10 Iteration: 285 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 290 Train loss: 0.025\n",
      "Epoch: 3/10 Iteration: 295 Train loss: 0.058\n",
      "Epoch: 3/10 Iteration: 300 Train loss: 0.053\n",
      "Val acc: 0.946\n",
      "Epoch: 4/10 Iteration: 305 Train loss: 0.022\n",
      "Epoch: 4/10 Iteration: 310 Train loss: 0.024\n",
      "Epoch: 4/10 Iteration: 315 Train loss: 0.060\n",
      "Epoch: 4/10 Iteration: 320 Train loss: 0.033\n",
      "Epoch: 4/10 Iteration: 325 Train loss: 0.068\n",
      "Val acc: 0.953\n",
      "Epoch: 4/10 Iteration: 330 Train loss: 0.034\n",
      "Epoch: 4/10 Iteration: 335 Train loss: 0.048\n",
      "Epoch: 4/10 Iteration: 340 Train loss: 0.030\n",
      "Epoch: 4/10 Iteration: 345 Train loss: 0.029\n",
      "Epoch: 4/10 Iteration: 350 Train loss: 0.017\n",
      "Val acc: 0.949\n",
      "Epoch: 4/10 Iteration: 355 Train loss: 0.046\n",
      "Epoch: 4/10 Iteration: 360 Train loss: 0.053\n",
      "Epoch: 4/10 Iteration: 365 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 370 Train loss: 0.020\n",
      "Epoch: 4/10 Iteration: 375 Train loss: 0.038\n",
      "Val acc: 0.951\n",
      "Epoch: 4/10 Iteration: 380 Train loss: 0.027\n",
      "Epoch: 5/10 Iteration: 385 Train loss: 0.026\n",
      "Epoch: 5/10 Iteration: 390 Train loss: 0.044\n",
      "Epoch: 5/10 Iteration: 395 Train loss: 0.034\n",
      "Epoch: 5/10 Iteration: 400 Train loss: 0.038\n",
      "Val acc: 0.950\n",
      "Epoch: 5/10 Iteration: 405 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 410 Train loss: 0.027\n",
      "Epoch: 5/10 Iteration: 415 Train loss: 0.034\n",
      "Epoch: 5/10 Iteration: 420 Train loss: 0.051\n",
      "Epoch: 5/10 Iteration: 425 Train loss: 0.048\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 430 Train loss: 0.025\n",
      "Epoch: 5/10 Iteration: 435 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 440 Train loss: 0.025\n",
      "Epoch: 5/10 Iteration: 445 Train loss: 0.052\n",
      "Epoch: 5/10 Iteration: 450 Train loss: 0.031\n",
      "Val acc: 0.944\n",
      "Epoch: 5/10 Iteration: 455 Train loss: 0.033\n",
      "Epoch: 6/10 Iteration: 460 Train loss: 0.019\n",
      "Epoch: 6/10 Iteration: 465 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 470 Train loss: 0.047\n",
      "Epoch: 6/10 Iteration: 475 Train loss: 0.021\n",
      "Val acc: 0.950\n",
      "Epoch: 6/10 Iteration: 480 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 485 Train loss: 0.053\n",
      "Epoch: 6/10 Iteration: 490 Train loss: 0.042\n",
      "Epoch: 6/10 Iteration: 495 Train loss: 0.055\n",
      "Epoch: 6/10 Iteration: 500 Train loss: 0.028\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 505 Train loss: 0.032\n",
      "Epoch: 6/10 Iteration: 510 Train loss: 0.027\n",
      "Epoch: 6/10 Iteration: 515 Train loss: 0.031\n",
      "Epoch: 6/10 Iteration: 520 Train loss: 0.045\n",
      "Epoch: 6/10 Iteration: 525 Train loss: 0.035\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 530 Train loss: 0.041\n",
      "Epoch: 7/10 Iteration: 535 Train loss: 0.046\n",
      "Epoch: 7/10 Iteration: 540 Train loss: 0.026\n",
      "Epoch: 7/10 Iteration: 545 Train loss: 0.033\n",
      "Epoch: 7/10 Iteration: 550 Train loss: 0.029\n",
      "Val acc: 0.950\n",
      "Epoch: 7/10 Iteration: 555 Train loss: 0.027\n",
      "Epoch: 7/10 Iteration: 560 Train loss: 0.044\n",
      "Epoch: 7/10 Iteration: 565 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 570 Train loss: 0.023\n",
      "Epoch: 7/10 Iteration: 575 Train loss: 0.041\n",
      "Val acc: 0.951\n",
      "Epoch: 7/10 Iteration: 580 Train loss: 0.020\n",
      "Epoch: 7/10 Iteration: 585 Train loss: 0.013\n",
      "Epoch: 7/10 Iteration: 590 Train loss: 0.023\n",
      "Epoch: 7/10 Iteration: 595 Train loss: 0.017\n",
      "Epoch: 7/10 Iteration: 600 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 7/10 Iteration: 605 Train loss: 0.050\n",
      "Epoch: 8/10 Iteration: 610 Train loss: 0.025\n",
      "Epoch: 8/10 Iteration: 615 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 620 Train loss: 0.044\n",
      "Epoch: 8/10 Iteration: 625 Train loss: 0.027\n",
      "Val acc: 0.950\n",
      "Epoch: 8/10 Iteration: 630 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 635 Train loss: 0.064\n",
      "Epoch: 8/10 Iteration: 640 Train loss: 0.027\n",
      "Epoch: 8/10 Iteration: 645 Train loss: 0.034\n",
      "Epoch: 8/10 Iteration: 650 Train loss: 0.034\n",
      "Val acc: 0.954\n",
      "Epoch: 8/10 Iteration: 655 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 660 Train loss: 0.028\n",
      "Epoch: 8/10 Iteration: 665 Train loss: 0.021\n",
      "Epoch: 8/10 Iteration: 670 Train loss: 0.015\n",
      "Epoch: 8/10 Iteration: 675 Train loss: 0.043\n",
      "Val acc: 0.950\n",
      "Epoch: 8/10 Iteration: 680 Train loss: 0.040\n",
      "Epoch: 9/10 Iteration: 685 Train loss: 0.053\n",
      "Epoch: 9/10 Iteration: 690 Train loss: 0.045\n",
      "Epoch: 9/10 Iteration: 695 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 700 Train loss: 0.033\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 705 Train loss: 0.066\n",
      "Epoch: 9/10 Iteration: 710 Train loss: 0.027\n",
      "Epoch: 9/10 Iteration: 715 Train loss: 0.044\n",
      "Epoch: 9/10 Iteration: 720 Train loss: 0.031\n",
      "Epoch: 9/10 Iteration: 725 Train loss: 0.021\n",
      "Val acc: 0.950\n",
      "Epoch: 9/10 Iteration: 730 Train loss: 0.019\n",
      "Epoch: 9/10 Iteration: 735 Train loss: 0.042\n",
      "Epoch: 9/10 Iteration: 740 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 745 Train loss: 0.033\n",
      "Epoch: 9/10 Iteration: 750 Train loss: 0.018\n",
      "Val acc: 0.948\n",
      "Epoch: 9/10 Iteration: 755 Train loss: 0.043\n",
      "Epoch: 9/10 Iteration: 760 Train loss: 0.034\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph, config=GPU_MEM_CONFIG) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(test_x, test_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(l1_x, l1_y, batch_size), 1): #can also use test_x, test_y\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference! (Ignore everything below here for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import expansion\n",
    "\n",
    "importlib.reload(expansion)\n",
    "expander = expansion.PatentLandscapeExpander(\n",
    "    seed_file,\n",
    "    bq_project=bq_project,\n",
    "    patent_dataset=patent_dataset,\n",
    "    num_antiseed=num_anti_seed_patents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe with cols Index(['publication_number'], dtype='object'), shape (133137, 1), to patents._tmp_training\n",
      "Completed loading temp table.\n",
      "Loading patent texts from provided publication numbers.\n"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: apiLimitExceeded, Message: API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mjobId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_reference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jobId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                     pageToken=page_token).execute()\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/oauth2client/util.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://www.googleapis.com/bigquery/v2/projects/patent-landscape-165715/queries/job_Fp1OMsWBelejjyoKkxXfmQPYNs4?alt=json&pageToken=BGEWZFUNLQAQAAASA4EAAEEAQCAAKGQIBDF6UAQQUCGQMIFQVYKQ%3D%3D%3D%3D returned \"API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f64ee6646dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#training_df[training_df.abstract_text.str.contains('machine')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ml1_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_training_data_from_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_patents_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'publication_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0ml1_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Seed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ml1_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/src/models/google-patent-models/expansion.py\u001b[0m in \u001b[0;36mload_training_data_from_pubs\u001b[0;34m(self, training_publications_df)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             configuration = {'query': {'useQueryCache': True, 'allowLargeResults': False}})\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_data_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, dialect, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, dialect, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m                              \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                              dialect=dialect)\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0mdataframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m                     pageToken=page_token).execute()\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_row\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_rows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 raise GenericGBQException(\n\u001b[0;32m--> 352\u001b[0;31m                     \"Reason: {0}, Message: {1}\".format(reason, message))\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGenericGBQException\u001b[0m: Reason: apiLimitExceeded, Message: API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table."
     ]
    }
   ],
   "source": [
    "\n",
    "#training_df[training_df.abstract_text.str.contains('learn') & training_df.abstract_text.str.contains('machine')]\n",
    "\n",
    "#training_df[training_df.abstract_text.str.contains('machine')]\n",
    "l1_texts = expander.load_training_data_from_pubs(l1_patents_df[['publication_number']])\n",
    "l1_texts['label'] = 'Seed'\n",
    "l1_texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def binary_prediction_idx(score):\n",
    "    if score < .5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def predictions_for_report(patent_model, validX, validY):\n",
    "    target_names = ['seed', 'antiseed']\n",
    "\n",
    "    prediction_scores = patent_model.predict(validX)\n",
    "    predictions = []\n",
    "    actual_y = []\n",
    "\n",
    "    for idx in range(0, len(prediction_scores)):\n",
    "        prediction = prediction_scores[idx]\n",
    "        actual = validY[idx]\n",
    "        predictions.append(binary_prediction_idx(prediction[0]))\n",
    "        actual_y.append(actual)\n",
    "\n",
    "    return predictions, actual_y, target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping series (tokenization etc.) for training.\n",
      "Padding sequences.\n",
      "Making predictions\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       seed       0.31      1.00      0.47      3070\n",
      "   antiseed       1.00      0.00      0.00      6930\n",
      "\n",
      "avg / total       0.79      0.31      0.14     10000\n",
      "\n",
      "[[3070    0]\n",
      " [6929    1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "l1_texts_subset = l1_texts.abstract_text[:10000]\n",
    "l1_texts_subset.reset_index(drop=True, inplace=True)\n",
    "l1_texts_subset.loc[len(l1_texts_subset)-1] = 'this abstract is about something entirely boring'\n",
    "\n",
    "l1_labels_subset = l1_texts.label[:10000]\n",
    "l1_labels_subset.reset_index(drop=True, inplace=True)\n",
    "l1_labels_subset.loc[len(l1_texts_subset)-1] = 'AntiSeed'\n",
    "\n",
    "print('Prepping series (tokenization etc.) for training.')\n",
    "l1_x, l1_y = prep_series_for_training(w2v_runtime, l1_texts_subset, l1_labels_subset)\n",
    "l1_x = np.array(l1_x)\n",
    "l1_y = np.array(l1_y)\n",
    "\n",
    "print('Padding sequences.')\n",
    "# Convert text idx into padded sequences\n",
    "l1_x = sequence.pad_sequences(\n",
    "        l1_x, maxlen=sequence_len, padding='pre', truncating='post')\n",
    "# Converting labels to binary vectors\n",
    "#l1_y = to_categorical(l1_y, nb_classes=2)\n",
    "\n",
    "print('Making predictions')\n",
    "l1_preds, actual_l1_y, target_names = predictions_for_report(model, l1_x, l1_y)\n",
    "\n",
    "cr = classification_report(l1_preds, actual_l1_y, target_names=target_names)\n",
    "cm = confusion_matrix(l1_preds, actual_l1_y)\n",
    "print(cr)\n",
    "print(cm)\n",
    "#l1_texts_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PredictionIdx Prediction\n",
       "0                 1   antiseed\n",
       "1                 1   antiseed\n",
       "2                 1   antiseed\n",
       "3                 0       seed\n",
       "4                 1   antiseed\n",
       "5                 1   antiseed\n",
       "6                 1   antiseed\n",
       "7                 1   antiseed\n",
       "8                 1   antiseed\n",
       "9                 1   antiseed\n",
       "10                1   antiseed\n",
       "11                1   antiseed\n",
       "12                1   antiseed\n",
       "13                1   antiseed\n",
       "14                1   antiseed\n",
       "15                0       seed\n",
       "16                1   antiseed\n",
       "17                1   antiseed\n",
       "18                1   antiseed\n",
       "19                1   antiseed\n",
       "20                1   antiseed\n",
       "21                0       seed\n",
       "22                1   antiseed\n",
       "23                0       seed\n",
       "24                0       seed\n",
       "25                1   antiseed\n",
       "26                1   antiseed\n",
       "27                0       seed\n",
       "28                0       seed\n",
       "29                1   antiseed\n",
       "...             ...        ...\n",
       "9970              1   antiseed\n",
       "9971              1   antiseed\n",
       "9972              1   antiseed\n",
       "9973              1   antiseed\n",
       "9974              0       seed\n",
       "9975              1   antiseed\n",
       "9976              1   antiseed\n",
       "9977              1   antiseed\n",
       "9978              0       seed\n",
       "9979              1   antiseed\n",
       "9980              1   antiseed\n",
       "9981              1   antiseed\n",
       "9982              0       seed\n",
       "9983              1   antiseed\n",
       "9984              0       seed\n",
       "9985              1   antiseed\n",
       "9986              1   antiseed\n",
       "9987              1   antiseed\n",
       "9988              1   antiseed\n",
       "9989              1   antiseed\n",
       "9990              1   antiseed\n",
       "9991              1   antiseed\n",
       "9992              1   antiseed\n",
       "9993              1   antiseed\n",
       "9994              1   antiseed\n",
       "9995              0       seed\n",
       "9996              1   antiseed\n",
       "9997              1   antiseed\n",
       "9998              0       seed\n",
       "9999              1   antiseed\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(l1_preds)\n",
    "l1_preds_df = pd.DataFrame(l1_preds, columns=['PredictionIdx'])\n",
    "l1_preds_df['Prediction'] = l1_preds_df.PredictionIdx.apply(label_id_to_text)\n",
    "l1_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The present invention relates to a method for detecting fetal Down syndrome (Trisomy 21), trisomy 13, trisomy 18 and other chromosomal anomalies during prenatal screening by analyzing a dried blood sample from a pregnant woman. More particularly the present invention relates to a method for improving detection efficiency in screening for the anomalies by measuring the amount of the free beta human chorionic gonadotropin (HCG) and nicked or fragmented or aberrant forms of free beta (HCG), all of which are referenced throughout this application as free beta (HCG) in dried blood samples from pregnant women.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_texts_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd3d8c6f978>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGaCAYAAAAl57hmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd28aebbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm_df = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_prediction(patent_model, text, expected_label, max_seq_len, mask_idx=0):\n",
    "    inf_series = pd.Series([text])\n",
    "    lab_series = pd.Series([label])\n",
    "\n",
    "    prepped_train, prepped_labels = \\\n",
    "        prep_series_for_training(\n",
    "            w2v_runtime=w2v_runtime,\n",
    "            labels_series=lab_series,\n",
    "            raw_series_text=inf_series)\n",
    "\n",
    "    # make sure we have the correct sequence length\n",
    "    #infX = pad_sequences(prepped_train, maxlen=max_seq_len, value=mask_idx)\n",
    "    infX = sequence.pad_sequences(\n",
    "        prepped_train, maxlen=sequence_len, padding='pre', truncating='post')\n",
    "    #prepped_train = np.array(prepped_train)\n",
    "\n",
    "    # actually make prediction\n",
    "    prediction = patent_model.predict(infX)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(filepath='checkpoints/keras_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: three dimensional mapping\n",
      "Prediction: [[ 0.98487407]]\n",
      "Predicted Label: antiseed, Actual Label: Seed\n"
     ]
    }
   ],
   "source": [
    "text = 'a really boring title that is super long and has nothing to do with the topic that im interested in. but has a bit of stuff about artificial neural networks and things i haven\\'t heard of?'\n",
    "text = 'three dimensional mapping'\n",
    "#text = l1_texts_subset[10]\n",
    "label = 'Seed'\n",
    "prediction = make_prediction(model, text, expected_label=label, max_seq_len=sequence_len)\n",
    "print('Text: {}'.format(text))\n",
    "#print('Tokenized/Integerized: {}'.format(prepped_train))\n",
    "#print('Padded: {}'.format(infX))\n",
    "print('Prediction: {}'.format(prediction))\n",
    "print('Predicted Label: {}, Actual Label: {}'.format(label_id_to_text(binary_prediction_idx(prediction)), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#oldidx = print_idx\n",
    "#print_idx = oldidx\n",
    "#print(model.predict([trainX[print_idx]]))\n",
    "#tensor_label_to_text(trainY[print_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pl35]",
   "language": "python",
   "name": "conda-env-pl35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
